================================================================================
                    DATA SECTION FOR THESIS
================================================================================
              SINHALA FAKE NEWS DETECTION: DATA COLLECTION AND PROCESSING
                                   December 2024
================================================================================



================================================================================
                    CHAPTER: DATA COLLECTION AND PREPROCESSING
================================================================================

This chapter describes the data used in this project. It explains where the
data came from, how it was collected, how it was processed, and how it was
stored. Understanding the data is important because the quality of a fake
news detection system depends heavily on the quality of its data.



================================================================================
1. INTRODUCTION TO THE DATA CHALLENGE
================================================================================

1.1 THE PROBLEM OF FINDING SINHALA DATA
---------------------------------------

One of the biggest challenges in this project was finding good data. For
English language, there are many publicly available datasets for fake news
detection:

- LIAR Dataset: Contains 12,800 labeled statements from PolitiFact
- FakeNewsNet: Contains thousands of fake and real news articles
- ISOT Fake News Dataset: Contains 21,417 real and 23,481 fake articles
- Kaggle Fake News Challenge: Competition-grade labeled dataset

But for Sinhala language, there are almost NO publicly available datasets.
This is because:

1. RESEARCH GAP: Not many researchers work on Sinhala NLP
   - Most NLP research focuses on English, Chinese, Spanish
   - Sri Lankan universities have limited NLP research programs
   - Very few published papers on Sinhala fake news detection

2. LABELING DIFFICULTY: Labeling requires Sinhala speakers
   - Annotators must read and understand Sinhala fluently
   - They must have knowledge of Sri Lankan current affairs
   - Professional fact-checkers are expensive
   - Volunteer annotators are hard to find

3. DATA AVAILABILITY: Sinhala content is limited online
   - Fewer Sinhala websites compared to English
   - Sinhala social media content is harder to collect
   - Many Sinhala news sites don't have good archives


1.2 MY SOLUTION: MULTI-SOURCE DATA COLLECTION
---------------------------------------------

To solve the data problem, I used a multi-source approach:

SOURCE TYPE 1: TRUSTED NEWS WEBSITES
- Collected articles from Lankadeepa and Hiru News
- These are professional news organizations
- Their content can be assumed TRUE with high confidence

SOURCE TYPE 2: SOCIAL MEDIA POSTS
- Collected from Twitter/X and Facebook
- Mix of real and fake information
- Required manual verification

SOURCE TYPE 3: EXISTING ANNOTATED COLLECTIONS
- Found some smaller annotated datasets
- Combined them with my collected data

This approach gave me enough data to build a working system while ensuring
diversity of sources and content types.



================================================================================
2. DETAILED DESCRIPTION OF DATA SOURCES
================================================================================


2.1 LANKADEEPA NEWS DATASET
===========================

FILE NAME: Lankadeepa_2019.csv
APPROXIMATE SIZE: 2,500 articles

2.1.1 About Lankadeepa Newspaper
--------------------------------

Lankadeepa (‡∂Ω‡∂Ç‡∂ö‡∑è‡∂Ø‡∑ì‡∂¥) is one of the oldest and most respected Sinhala
newspapers in Sri Lanka. Here are important facts about the source:

ESTABLISHMENT: Founded in 1947
OWNERSHIP: Owned by Wijeya Newspapers Limited
CIRCULATION: One of the highest-circulating Sinhala newspapers
EDITORIAL STANDARDS: Professional editors, fact-checkers, and sub-editors
REPUTATION: Generally considered reliable and trustworthy
WEBSITE: www.lankadeepa.lk

The newspaper covers:
- Politics and government
- Business and economy
- Sports (especially cricket)
- Entertainment and arts
- Local news from all districts
- International news

2.1.2 How Data Was Collected
----------------------------

The data was collected using web scraping. Web scraping means using a
computer program to automatically visit websites and extract information.

SCRAPING PROCESS:
1. A Python script was written using the 'requests' library
2. The script visited each article page on lankadeepa.lk
3. BeautifulSoup library was used to parse the HTML
4. Article title and content were extracted from specific HTML tags
5. Data was saved to a CSV file

SCRAPING CONSIDERATIONS:
- robots.txt was respected (scraped only allowed pages)
- Rate limiting was applied (wait between requests)
- Only publicly available articles were scraped
- No login-required content was accessed

TIME PERIOD: Articles from 2019 were collected
COVERAGE: Multiple categories (politics, sports, business, etc.)

2.1.3 Data Structure
--------------------

The CSV file has the following columns:

+------------+--------+------------------------------------------------+
| Column     | Type   | Description                                    |
+------------+--------+------------------------------------------------+
| title      | String | The headline of the news article               |
|            |        | Example: "‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í ‡∂±‡∑Ä ‡∂Ö‡∂∏‡∑è‡∂≠‡∑ä‚Äç‡∂∫ ‡∂∏‡∂´‡∑ä‡∂©‡∂Ω‡∂∫ ‡∂±‡∂∏‡∑ä ‡∂ö‡∂ª‡∂∫‡∑í"    |
+------------+--------+------------------------------------------------+
| content    | String | The full text of the article body              |
|            |        | Usually 200-800 words in Sinhala               |
+------------+--------+------------------------------------------------+

2.1.4 Label Assignment
----------------------

LABEL: ALL ARTICLES MARKED AS "TRUE"

JUSTIFICATION:
1. Lankadeepa is a professional news organization
2. They employ trained journalists
3. Articles go through editorial review
4. The newspaper has a 75+ year reputation to protect
5. Factual errors are rare and usually corrected
6. There is no incentive to publish false information

LIMITATIONS OF THIS ASSUMPTION:
- Some articles may contain minor errors
- Opinion pieces may be biased
- Breaking news may lack full verification
- However, intentionally false information is extremely rare

2.1.5 Sample Data
-----------------

SAMPLE 1:
Title: "‡∂∏‡∑Ñ ‡∂∂‡∑ê‡∂Ç‡∂ö‡∑î‡∑Ä ‡∂¥‡∑ú‡∂Ω‡∑ì ‡∂Ö‡∂±‡∑î‡∂¥‡∑è‡∂≠‡∑í‡∂ö ‡∑É‡∂Ç‡∑Å‡∑ù‡∂∞‡∂±‡∂∫ ‡∂ö‡∂ª‡∂∫‡∑í"
Content: "‡∑Å‡∑ä‚Äç‡∂ª‡∑ì ‡∂Ω‡∂Ç‡∂ö‡∑è ‡∂∏‡∑Ñ ‡∂∂‡∑ê‡∂Ç‡∂ö‡∑î‡∑Ä ‡∑Ä‡∑í‡∑É‡∑í‡∂±‡∑ä ‡∂Ö‡∂Ø (2019.01.15) ‡∂Ø‡∑í‡∂± ‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∂¥‡∂≠‡∑ä‡∂≠‡∑í
‡∂¥‡∑ú‡∂Ω‡∑ì ‡∂Ö‡∂±‡∑î‡∂¥‡∑è‡∂≠‡∑í‡∂ö‡∂∫ ‡∑É‡∑í‡∂∫‡∂∫‡∂ß 8.5 ‡∂Ø‡∂ö‡∑ä‡∑Ä‡∑è ‡∂â‡∑Ñ‡∑Ö ‡∂Ø‡∑ê‡∂∏‡∑ì‡∂∏‡∂ß ‡∂≠‡∑ì‡∂ª‡∂´‡∂∫ ‡∂ö‡∂ª ‡∂á‡∂≠..."
Label: true
Source: Lankadeepa

Translation: "Central Bank revises interest rates"



2.2 HIRU NEWS DATASET
=====================

FILE NAME: hirunews_2023_02_to_2023_06_1000_cleaned_labeled.csv
APPROXIMATE SIZE: 1,000 articles

2.2.1 About Hiru News
---------------------

Hiru News (‡∑Ñ‡∑í‡∂ª‡∑î News) is one of the most popular news sources in Sri Lanka.
It is part of the Hiru TV channel, which is owned by Asia Broadcasting
Corporation Private Limited (ABC).

ESTABLISHMENT: Hiru TV launched in 2011
PLATFORMS: TV channel, website, YouTube, Facebook, mobile app
VIEWERSHIP: Among the highest-rated news programs in Sri Lanka
WEBSITE: www.hirunews.lk
LANGUAGES: Primarily Sinhala, some English content

Hiru News is known for:
- Breaking news coverage
- Political news
- Entertainment news
- Viral content
- High engagement on social media

2.2.2 How Data Was Collected
----------------------------

A custom web scraper was built specifically for the Hiru News website.

TECHNICAL DETAILS:

Libraries used:
- requests: For downloading web pages
- BeautifulSoup: For parsing HTML
- lxml: For faster HTML parsing
- pandas: For storing data

Scraping steps:
1. Start at the Hiru News archive page
2. Navigate to each date in the range (Feb 2023 - June 2023)
3. Collect all article links for each day
4. Visit each article page
5. Extract title and content from specific HTML tags
6. Clean HTML tags from content
7. Save to CSV file

Rate limiting:
- 1 second delay between requests
- Random additional delay (0-2 seconds) to appear natural
- Maximum 1,000 articles collected to respect server load

2.2.3 Data Structure
--------------------

+------------+--------+------------------------------------------------+
| Column     | Type   | Description                                    |
+------------+--------+------------------------------------------------+
| title      | String | News headline                                  |
+------------+--------+------------------------------------------------+
| content    | String | Full article text (cleaned of HTML)            |
+------------+--------+------------------------------------------------+
| verified   | String | "TRUE" or "FALSE" - verification status        |
+------------+--------+------------------------------------------------+

2.2.4 Why Feb-June 2023?
------------------------

This time period was chosen because:
1. Recent enough to be relevant
2. Sri Lanka was experiencing major political events
3. Economic crisis was ongoing (lots of claims to verify)
4. COVID-19 pandemic response was still in news
5. Good mix of political, economic, and social news

2.2.5 Label Assignment
----------------------

LABEL: MARKED AS "TRUE" (from verified column)

Similar reasoning to Lankadeepa - Hiru News is a professional news
organization with editorial oversight. While they may have different
political leanings than Lankadeepa, their factual reporting is generally
reliable.



2.3 ANNOTATED FAKE NEWS DATASET
===============================

FILE NAME: FakeNews_Annotated.csv
APPROXIMATE SIZE: 500 records

2.3.1 Why This Dataset Is Special
---------------------------------

This is the most valuable dataset because it contains BOTH true AND false
examples. The other datasets (Lankadeepa, Hiru) are mostly all true.
For a fake news detection system to work, it needs to learn what fake
news looks like.

2.3.2 Source of the Data
------------------------

The texts in this dataset came from various sources:

SOURCE 1: WHATSAPP FORWARDS
- Viral messages forwarded many times
- Often contain sensational claims
- Many are false or misleading
- Examples: Health cures, political conspiracies

SOURCE 2: FACEBOOK POSTS
- Public posts about current events
- Some from fake news pages
- Some from individuals sharing rumors
- Screenshots of posts were collected

SOURCE 3: ONLINE RUMORS
- Claims circulating on forums
- Comments on news articles
- Viral social media content

2.3.3 Annotation Process
------------------------

Each text was manually verified by human annotators:

STEP 1: READ THE CLAIM
- Annotator reads the text carefully
- Identifies the main factual claim

STEP 2: SEARCH FOR EVIDENCE
- Google search in Sinhala and English
- Check trusted news websites
- Look for official statements
- Check AFP Fact Check Sri Lanka
- Check fact-checking websites

STEP 3: MAKE DECISION
- If claim matches trusted sources ‚Üí Label = 1 (TRUE)
- If claim contradicts trusted sources ‚Üí Label = 0 (FALSE)
- If cannot verify either way ‚Üí Excluded from dataset

STEP 4: RECORD LABEL
- Label saved in CSV file
- 1 = TRUE (verified as accurate)
- 0 = FALSE (verified as inaccurate)

2.3.4 Data Structure
--------------------

+------------+--------+------------------------------------------------+
| Column     | Type   | Description                                    |
+------------+--------+------------------------------------------------+
| Text       | String | The claim text in Sinhala                      |
+------------+--------+------------------------------------------------+
| Label      | Integer| 0 = False, 1 = True                           |
+------------+--------+------------------------------------------------+

2.3.5 Types of False Information Found
--------------------------------------

The dataset contains various types of misinformation:

TYPE 1: HEALTH MISINFORMATION
- Fake COVID cures ("Boil king coconut water to cure COVID")
- Vaccine myths ("Vaccine contains microchips")
- Fake medical advice

TYPE 2: POLITICAL MISINFORMATION
- Fake quotes attributed to politicians
- False claims about government actions
- Manipulated images of politicians

TYPE 3: DISASTER RUMORS
- Exaggerated earthquake/tsunami warnings
- Fake death tolls
- Non-existent emergencies

TYPE 4: SOCIAL RUMORS
- False claims about public figures
- Fake crime reports
- Fabricated events

2.3.6 Sample Data
-----------------

SAMPLE 1 (FALSE):
Text: "‡∑Ñ‡∑ô‡∂ß ‡∑É‡∑í‡∂ß ‡∂ª‡∂ß ‡∂¥‡∑î‡∂ª‡∑è ‡∂á‡∂≥‡∑í‡∂ª‡∑í ‡∂±‡∑ì‡∂≠‡∑í‡∂∫ ‡∂¥‡∂±‡∑Ä‡∂± ‡∂∂‡∑Ä ‡∂Ü‡∂ª‡∂ö‡∑ä‡∑Ç‡∂ö ‡∂Ö‡∂∏‡∑è‡∂≠‡∑ä‚Äç‡∂∫‡∂Ç‡∑Å‡∂∫ ‡∂±‡∑í‡∑Ä‡∑ö‡∂Ø‡∂±‡∂∫ ‡∂ö‡∑Ö‡∑è"
Translation: "Defense Ministry announced curfew will be imposed nationwide from tomorrow"
Label: 0 (FALSE - this was a viral hoax)

SAMPLE 2 (TRUE):
Text: "2023 ‡∂Ö‡∂∫‡∑Ä‡∑ê‡∂∫ ‡∂Ö‡∂Ø ‡∂¥‡∑è‡∂ª‡∑ä‡∂Ω‡∑í‡∂∏‡∑ö‡∂±‡∑ä‡∂≠‡∑î‡∑Ä‡∂ß ‡∂â‡∂Ø‡∑í‡∂ª‡∑í‡∂¥‡∂≠‡∑ä ‡∂ö‡∑Ö‡∑è"
Translation: "2023 Budget was presented in Parliament today"
Label: 1 (TRUE - verified from multiple sources)



2.4 LEGITIMATE NEWS POSTS DATASET
=================================

FILE NAME: NewsPosts_Legit.csv
APPROXIMATE SIZE: 500 posts

2.4.1 About This Dataset
------------------------

This dataset contains social media posts that share legitimate news.
The posts were collected from verified accounts of news organizations.

2.4.2 Sources
-------------

POST SOURCES:
- Official Facebook pages of news organizations
- Verified Twitter accounts of journalists
- Official news app notifications

EXAMPLES OF ACCOUNTS:
- Hiru News Official Facebook
- Lankadeepa Official Page
- Sri Lanka Mirror
- News First Sri Lanka

2.4.3 Why Include This?
-----------------------

This dataset is important because:
1. Social media posts have different style than full articles
2. They are shorter and more informal
3. They often use hashtags and mentions
4. They may include Singlish (romanized Sinhala)
5. The system needs to handle this format

2.4.4 Data Structure
--------------------

+------------+--------+------------------------------------------------+
| Column     | Type   | Description                                    |
+------------+--------+------------------------------------------------+
| content    | String | The social media post text                     |
+------------+--------+------------------------------------------------+

(All posts are labeled TRUE as they come from verified accounts)



2.5 TWITTER POSTS DATASET
=========================

FILE NAME: TwitterPosts_Labeled.csv
APPROXIMATE SIZE: 1,000 posts

2.5.1 About This Dataset
------------------------

This dataset contains tweets about Sri Lankan news and events. Twitter
(now X) is an important source because:
- Information spreads very fast
- Both real and fake news appears
- Users share and comment on news
- It reflects public discourse

2.5.2 How Data Was Collected
----------------------------

DATA COLLECTION METHOD:
Tweets were collected using the Twitter Academic Research API (before it
became paid-only in 2023).

SEARCH QUERIES USED:
- #SriLanka
- #lka
- #srilanka
- Sinhala keywords: ‡∑Å‡∑ä‚Äç‡∂ª‡∑ì ‡∂Ω‡∂Ç‡∂ö‡∑è, ‡∂ª‡∂¢‡∂∫, ‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í

TIME PERIOD: 2022-2023

FILTERING:
- Only tweets with more than 5 retweets (to capture viral content)
- Exclude tweets with only links (need text content)
- Exclude non-Sinhala tweets

2.5.3 Annotation Process
------------------------

Similar to FakeNews_Annotated dataset:

1. Each tweet was read by annotator
2. The claim was fact-checked
3. Label assigned: 1=TRUE, 0=FALSE

2.5.4 Challenges with Twitter Data
----------------------------------

CHALLENGE 1: MIXED LANGUAGE
Tweets often mix Sinhala, English, and Singlish:
"Fuel prices ‡∂â‡∑Ñ‡∂Ω ‡∂ú‡∑í‡∂∫‡∑è today, Rs. 50 ‡∑Ä‡∑ê‡∂©‡∑í‡∂∫‡∑í"

CHALLENGE 2: SHORT TEXT
Tweets are limited to 280 characters, which makes them harder to
fact-check due to lack of context.

CHALLENGE 3: INFORMAL LANGUAGE
- Abbreviations: "amw" (anyway)
- Emoji usage: üò°üî•
- Hashtags embedded in text

SOLUTION:
The preprocessing pipeline handles these challenges with special
normalization for mixed-language content.



================================================================================
3. DATA MERGING PROCESS - DETAILED EXPLANATION
================================================================================

3.1 WHY MERGE ALL DATA?
-----------------------

Each dataset on its own has limitations:

LANKADEEPA/HIRU: Only TRUE examples, no FALSE
ANNOTATED: Both labels but small size
TWITTER: Good variety but needs cleaning

By merging all datasets, we get:
- Both TRUE and FALSE examples
- Variety of text styles (formal articles, informal posts)
- Multiple sources (to avoid source bias)
- Larger total size for better results


3.2 THE MERGING ALGORITHM (DETAILED)
------------------------------------

I created a Python script that combines all CSV files. Here is exactly
what it does:

FUNCTION 1: load_lankadeepa()
-----------------------------
def load_lankadeepa():
    """Load Lankadeepa - all TRUE."""
    df = pd.read_csv("Lankadeepa_2019.csv")
    records = []
    
    for each_row in df:
        # Try 'content' column first, then 'cleaned_t'
        text = row.get('content') or row.get('cleaned_t')
        
        # Filter short texts
        if len(text) > 30:
            records.append({
                "text": text,
                "title": row.get('title', ''),
                "source": "Lankadeepa",
                "label": "true"  # All Lankadeepa is TRUE
            })
    
    return records

FUNCTION 2: load_fakenews_annotated()
-------------------------------------
def load_fakenews_annotated():
    """Load annotated data - has both TRUE and FALSE."""
    df = pd.read_csv("FakeNews_Annotated.csv")
    records = []
    
    for each_row in df:
        text = row.get('Text')
        
        # Convert numeric label to string
        label_value = row.get('Label')
        if label_value == 1:
            label = "true"
        else:
            label = "false"
        
        if len(text) > 30:
            records.append({
                "text": text,
                "source": "Twitter",
                "label": label
            })
    
    return records

SIMILAR FUNCTIONS FOR OTHER DATASETS...


3.3 HANDLING DIFFERENT COLUMN NAMES
-----------------------------------

Each CSV file uses different column names. Here's how I handled this:

+------------------+-------------------+-----------------------------+
| Dataset          | Text Column       | How to Access               |
+------------------+-------------------+-----------------------------+
| Lankadeepa       | content           | row.get('content')          |
|                  | OR cleaned_t      | row.get('cleaned_t')        |
+------------------+-------------------+-----------------------------+
| NewsPosts        | content           | row.get('content')          |
+------------------+-------------------+-----------------------------+
| FakeNews         | Text              | row.get('Text')             |
+------------------+-------------------+-----------------------------+
| Hirunews         | content           | row.get('content')          |
|                  | OR text           | row.get('text')             |
+------------------+-------------------+-----------------------------+
| Twitter          | Text              | row.get('Text')             |
+------------------+-------------------+-----------------------------+

SOLUTION CODE:
text = row.get('content', row.get('Text', row.get('cleaned_t', '')))

This tries each column name and uses whichever one exists.


3.4 HANDLING DIFFERENT LABEL FORMATS
------------------------------------

Labels come in different formats:

+------------------+-------------------+-----------------------------+
| Dataset          | Label Format      | Conversion                  |
+------------------+-------------------+-----------------------------+
| Lankadeepa       | (no column)       | Always "true"               |
+------------------+-------------------+-----------------------------+
| NewsPosts        | (no column)       | Always "true"               |
+------------------+-------------------+-----------------------------+
| FakeNews         | 0 or 1            | 0‚Üí"false", 1‚Üí"true"         |
+------------------+-------------------+-----------------------------+
| Hirunews         | "TRUE"/"FALSE"    | .lower() to "true"/"false"  |
+------------------+-------------------+-----------------------------+
| Twitter          | 0 or 1            | 0‚Üí"false", 1‚Üí"true"         |
+------------------+-------------------+-----------------------------+

STANDARDIZATION:
All labels are converted to lowercase strings: "true" or "false"


3.5 OUTPUT FORMAT: JSONL
------------------------

The merged data is saved in JSONL format. JSONL means:
- JSON = JavaScript Object Notation
- L = Lines
- Each line is a separate JSON object

EXAMPLE unified_labeled.jsonl:
{"id": "0", "text": "‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í...", "label": "true", "source": "Lankadeepa"}
{"id": "1", "text": "‡∂Ö‡∂Ø ‡∑É‡∑í‡∂ß...", "label": "false", "source": "Twitter"}
{"id": "2", "text": "‡∂∏‡∑Ñ ‡∂∂‡∑ê‡∂Ç‡∂ö‡∑î‡∑Ä...", "label": "true", "source": "Hiru News"}

WHY JSONL?
1. Can process line by line (memory efficient)
2. Each record is independent
3. Easy to read and debug
4. Standard format supported by many tools



================================================================================
4. TEXT PREPROCESSING - DETAILED EXPLANATION
================================================================================

Raw text from websites and social media contains many issues that need
to be fixed before the text can be used. This section explains each
preprocessing step in detail.


4.1 TEXT NORMALIZATION
======================

4.1.1 What Is Normalization?
----------------------------

Text normalization means converting text to a standard, consistent format.
Without normalization, the same word might appear different ways:

PROBLEM EXAMPLE:
"‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í" might be stored as different Unicode sequences
"‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í    ‡∂ú‡∑í‡∂∫‡∑è" has extra spaces
"<p>‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í</p>" has HTML tags

4.1.2 Normalization Steps
-------------------------

STEP 1: UNICODE NORMALIZATION
- Convert to NFC (Normalized Form Composed)
- Ensure consistent character representation
- Example: Combining characters are properly composed

STEP 2: WHITESPACE CLEANING
- Multiple spaces ‚Üí single space
- Remove tabs and newlines
- Trim leading/trailing whitespace
- Example: "Hello    World" ‚Üí "Hello World"

STEP 3: HTML ENTITY DECODING
- &amp; ‚Üí &
- &nbsp; ‚Üí space
- &lt; ‚Üí <
- Example: "Price &gt; 100" ‚Üí "Price > 100"

STEP 4: CONTROL CHARACTER REMOVAL
- Remove invisible control characters
- Remove zero-width spaces
- Remove right-to-left marks

STEP 5: KEEP ONLY VALID CHARACTERS
- Sinhala Unicode range (0D80-0DFF)
- English letters (a-z, A-Z)
- Numbers (0-9)
- Basic punctuation (., ?, !, ")


4.2 SENTENCE TOKENIZATION
=========================

4.2.1 What Is Sentence Tokenization?
------------------------------------

Sentence tokenization means splitting text into individual sentences.

EXAMPLE:
Input: "‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í ‡∂ú‡∑í‡∂∫‡∑è. ‡∂î‡∑Ñ‡∑î ‡∑Ñ‡∂∏‡∑î‡∑Ä‡∑ì‡∂∏‡∂ö‡∑ä ‡∂¥‡∑ê‡∑Ä‡∑ê‡∂≠‡∑ä‡∑Ä‡∑ñ‡∑Ä‡∑è."
Output: ["‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í ‡∂ú‡∑í‡∂∫‡∑è.", "‡∂î‡∑Ñ‡∑î ‡∑Ñ‡∂∏‡∑î‡∑Ä‡∑ì‡∂∏‡∂ö‡∑ä ‡∂¥‡∑ê‡∑Ä‡∑ê‡∂≠‡∑ä‡∑Ä‡∑ñ‡∑Ä‡∑è."]

4.2.2 Why Is It Needed?
-----------------------

1. Claims are usually single sentences
2. NER works better on sentences
3. Easier to find specific information
4. Vector embeddings can be more precise

4.2.3 Challenges with Sinhala
-----------------------------

CHALLENGE 1: ABBREVIATIONS
"Dr. ‡∑É‡∑í‡∂Ω‡∑ä‡∑Ä‡∑è ‡∂ú‡∑í‡∂∫‡∑è" should NOT split after "Dr."
"Rs. 1000" should NOT split after "Rs."

CHALLENGE 2: QUOTES
"‡∂î‡∑Ñ‡∑î '‡∂∏‡∂∏ ‡∂∫‡∂±‡∑Ä‡∑è. ‡∂¥‡∑É‡∑î‡∑Ä ‡∂ë‡∂±‡∑Ä‡∑è.' ‡∂ö‡∑ì‡∑Ä‡∑è"
Should not split the quoted text

CHALLENGE 3: DECIMAL NUMBERS
"1.5 million" should not split after 1

4.2.4 Solution Algorithm
------------------------

The sentence splitter:
1. Find all period/question marks
2. Check if next character is space + uppercase/Sinhala
3. If yes, this is a sentence boundary
4. If no, continue (probably abbreviation)


4.3 WORD TOKENIZATION
=====================

4.3.1 What Is Word Tokenization?
--------------------------------

Word tokenization means splitting text into individual words (tokens).

EXAMPLE:
Input: "‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í ‡∂ª‡∑è‡∂¢‡∂¥‡∂ö‡∑ä‡∑Ç ‡∂ú‡∑í‡∂∫‡∑è"
Output: ["‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í", "‡∂ª‡∑è‡∂¢‡∂¥‡∂ö‡∑ä‡∑Ç", "‡∂ú‡∑í‡∂∫‡∑è"]

4.3.2 Challenges with Sinhala
-----------------------------

CHALLENGE 1: AGGLUTINATION
Sinhala words can have suffixes attached:
- "‡∂ö‡∂ª‡∂±‡∑Ä‡∑è" = "do" (verb)
- "‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö" = "do" + specific suffix
- "‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö‡∂∫" = "do" + emphatic suffix

CHALLENGE 2: COMPOUND WORDS
Some concepts are written as one word:
- "‡∑Ä‡∑í‡∑Å‡∑ä‡∑Ä‡∑Ä‡∑í‡∂Ø‡∑ä‚Äç‡∂∫‡∑è‡∂Ω‡∂∫" = "university" (literally "knowledge-building")
- "‡∂Ü‡∂∫‡∂≠‡∂±‡∂∫" = "institution"

CHALLENGE 3: NO CLEAR SPACES
Some Sinhala writing doesn't consistently use spaces:
- "‡∂ú‡∑í‡∂∫‡∑è‡∂Ø‡∑ê‡∂±‡∑ä‡∂∏‡∂∏" could mean "went + now + also"

4.3.3 Solution: Sinling Library
-------------------------------

I used the 'sinling' library which is designed for Sinhala:

import sinling
tokenizer = sinling.tokenizer

tokens = tokenizer.tokenize("‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í ‡∂ª‡∑è‡∂¢‡∂¥‡∂ö‡∑ä‡∑Ç ‡∂ú‡∑í‡∂∫‡∑è")
# Returns: ["‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í", "‡∂ª‡∑è‡∂¢‡∂¥‡∂ö‡∑ä‡∑Ç", "‡∂ú‡∑í‡∂∫‡∑è"]

The library handles:
- Sinhala script properly
- Common compounds
- Suffixes and affixes


4.4 PART-OF-SPEECH (POS) TAGGING
================================

4.4.1 What Is POS Tagging?
--------------------------

POS tagging assigns a grammatical label to each word:

+-----+------------------------+--------------------------------+
| Tag | Meaning                | Example                        |
+-----+------------------------+--------------------------------+
| NN  | Common Noun            | ‡∂¥‡∑ú‡∂≠ (book), ‡∂ú‡∑É (tree)          |
| NNP | Proper Noun            | ‡∂ª‡∂±‡∑í‡∂Ω‡∑ä (Ranil), ‡∂ö‡∑ú‡∑Ö‡∂π (Colombo)    |
| VB  | Verb                   | ‡∂ú‡∑í‡∂∫‡∑è (went), ‡∂ö‡∑Ö‡∑è (did)          |
| JJ  | Adjective              | ‡∂Ω‡∑É‡∑ä‡∑É‡∂± (beautiful)               |
| RB  | Adverb                 | ‡∂â‡∂ö‡∑ä‡∂∏‡∂±‡∑í‡∂±‡∑ä (quickly)              |
| RP  | Particle               | ‡∂∏ (emphasis), ‡∂≠‡∑ä (also)         |
| PP  | Postposition           | ‡∂ë‡∂ö‡∑ä‡∂ö (with), ‡∂ú‡∑ê‡∂± (about)         |
+-----+------------------------+--------------------------------+

4.4.2 Why Is POS Tagging Useful for Fake News?
----------------------------------------------

1. CLAIM STRUCTURE: Claims often have specific patterns
   "X said Y" = SUBJECT + VERB + OBJECT
   Finding this pattern helps identify claims

2. EMOTIONAL WORDS: Fake news often uses more adjectives
   "Shocking discovery" vs "Scientists announce"
   More adjectives might indicate sensationalism

3. PROPER NOUNS: Real news names specific people/places
   "President Wickremesinghe announced in Colombo"
   Fake news sometimes uses vague references

4.4.3 Example POS Tagging
-------------------------

Text: "‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í ‡∂ª‡∂±‡∑í‡∂Ω‡∑ä ‡∂Ö‡∂Ø ‡∂ö‡∑ú‡∑Ö‡∂π ‡∂ú‡∑í‡∂∫‡∑è"
Translation: "President Ranil went to Colombo today"

POS Tags:
+------------+-----+------------------------+
| Word       | Tag | Meaning                |
+------------+-----+------------------------+
| ‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í   | NN  | President (common noun)|
| ‡∂ª‡∂±‡∑í‡∂Ω‡∑ä      | NNP | Ranil (proper noun)    |
| ‡∂Ö‡∂Ø         | RB  | today (adverb)         |
| ‡∂ö‡∑ú‡∑Ö‡∂π       | NNP | Colombo (proper noun)  |
| ‡∂ú‡∑í‡∂∫‡∑è       | VB  | went (verb)            |
+------------+-----+------------------------+


4.5 NAMED ENTITY RECOGNITION (NER)
==================================

4.5.1 What Is NER?
------------------

NER finds and categorizes named entities in text:

+-------------+------------------------------------------+
| Entity Type | Examples                                 |
+-------------+------------------------------------------+
| PERSON      | ‡∂ú‡∑ù‡∂®‡∑è‡∂∑‡∂∫, ‡∂ª‡∂±‡∑í‡∂Ω‡∑ä, ‡∂∏‡∑Ñ‡∑í‡∂±‡∑ä‡∂Ø                      |
| LOCATION    | ‡∂ö‡∑ú‡∑Ö‡∂π, ‡∂ú‡∑è‡∂Ω‡∑ä‡∂Ω, ‡∂∏‡∑è‡∂≠‡∂ª                          |
| ORGANIZATION| ‡∂∏‡∑Ñ ‡∂∂‡∑ê‡∂Ç‡∂ö‡∑î‡∑Ä, ‡∂ª‡∂¢‡∂∫, ‡∂¥‡∑ä‚Äç‡∂ª‡∂ö‡∑è‡∑Å‡∂±                    |
| DATE        | ‡∂Ö‡∂Ø, ‡∂ä‡∂∫‡∑ö, 2023 ‡∂¢‡∂±‡∑Ä‡∑è‡∂ª‡∑í                      |
| MONEY       | ‡∂ª‡∑î‡∂¥‡∑í‡∂∫‡∂Ω‡∑ä 1000, Rs. 50                     |
+-------------+------------------------------------------+

4.5.2 Why NER Is Useful
-----------------------

1. VERIFIABILITY: Claims with specific names are easier to verify
   "President Ranil said X" can be checked
   "Someone said X" is harder to check

2. CONTEXT: Entities provide context
   "Colombo" tells us the location
   "2023" tells us the time

3. MATCHING: Find related articles
   If claim mentions "Ranil", search for other articles about "Ranil"

4.5.3 Example NER Output
------------------------

Text: "‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í ‡∂ª‡∂±‡∑í‡∂Ω‡∑ä ‡∂ä‡∂∫‡∑ö ‡∂ú‡∑è‡∂Ω‡∑ä‡∂Ω ‡∂ú‡∑í‡∂∫‡∑è"

NER Output:
{
    "PERSON": ["‡∂ª‡∂±‡∑í‡∂Ω‡∑ä"],
    "LOCATION": ["‡∂ú‡∑è‡∂Ω‡∑ä‡∂Ω"],
    "DATE": ["‡∂ä‡∂∫‡∑ö"]
}


4.6 CLAIM INDICATOR DETECTION
=============================

4.6.1 What Are Claim Indicators?
--------------------------------

Claim indicators are words that suggest a text contains a fact-checkable
claim. Not all text contains claims - some is opinion, some is questions,
some is description.

SINHALA CLAIM INDICATOR WORDS:

+----------+-----------------+----------------------------------+
| Word     | English         | What It Indicates                |
+----------+-----------------+----------------------------------+
| ‡∂Ö‡∂±‡∑î‡∑Ä     | according to    | Attributed claim                 |
| ‡∂ö‡∑í‡∂∫‡∑è     | said that       | Reported speech                  |
| ‡∂¥‡∑Ä‡∑É‡∂∫‡∑í    | says            | Quotation                        |
| ‡∂∫‡∑ê‡∂∫‡∑í     | reportedly      | Reported claim                   |
| ‡∂∂‡∑Ä       | that            | Embedded claim                   |
| ‡∂¥‡∑ä‚Äç‡∂ª‡∂ö‡∑è‡∑Å ‡∂ö‡∑Ö‡∑è| stated          | Official statement               |
| ‡∂±‡∑í‡∑Ä‡∑ö‡∂Ø‡∂±‡∂∫ ‡∂ö‡∑Ö‡∑è| announced      | Official announcement            |
| ‡∑Ä‡∑è‡∂ª‡∑ä‡∂≠‡∑è ‡∑Ä‡∑ö| reported        | News report                      |
+----------+-----------------+----------------------------------+

4.6.2 Example Detection
-----------------------

Text 1: "‡∂¢‡∂±‡∑è‡∂∞‡∑í‡∂¥‡∂≠‡∑í ‡∂¥‡∑ê‡∑Ä‡∑É‡∑ñ ‡∂∂‡∑Ä ‡∑Ä‡∑è‡∂ª‡∑ä‡∂≠‡∑è ‡∑Ä‡∑ö"
Translation: "It is reported that the President said"
Indicators found: ["‡∂¥‡∑ê‡∑Ä‡∑É‡∑ñ", "‡∂∂‡∑Ä", "‡∑Ä‡∑è‡∂ª‡∑ä‡∂≠‡∑è"]
Has claim: TRUE

Text 2: "‡∂Ö‡∂Ø ‡∂ö‡∑è‡∂Ω‡∂ú‡∑î‡∂´‡∂∫ ‡∂Ω‡∑É‡∑ä‡∑É‡∂±‡∂∫‡∑í"
Translation: "The weather is nice today"
Indicators found: []
Has claim: FALSE (this is observation, not a claim)


4.7 NEGATION DETECTION
======================

4.7.1 Why Detect Negation?
--------------------------

Negation completely changes the meaning of a sentence:

POSITIVE: "Fuel prices will increase"
NEGATIVE: "Fuel prices will NOT increase"

These are opposite claims. The system needs to know if negation is present.

4.7.2 Sinhala Negation Words
----------------------------

+-----------+----------------+----------------------------+
| Word      | English        | Example                    |
+-----------+----------------+----------------------------+
| ‡∂±‡∑ê‡∂≠       | not/no         | "‡∂î‡∑Ñ‡∑î ‡∂ú‡∑í‡∂∫‡∑ö ‡∂±‡∑ê‡∂≠" (he did not go)|
| ‡∂±‡∑ê‡∑Ñ‡∑ê      | no (informal)  | "‡∂í‡∂ö ‡∑Ñ‡∑ú‡∂≥ ‡∂±‡∑ê‡∑Ñ‡∑ê" (that's not good)|
| ‡∂±‡∑ú‡∑Ä‡∑ö      | is not         | "‡∂∏‡∑ö‡∂ö ‡∑É‡∂≠‡∑ä‚Äç‡∂∫ ‡∂±‡∑ú‡∑Ä‡∑ö" (this is not true)|
| ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂±‡∑ê‡∂≠  | cannot         | "‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂±‡∑ê‡∂≠" (cannot do)  |
| ‡∂±‡∑ú-       | not (prefix)   | "‡∂±‡∑ú‡∑É‡∑ê‡∂Ω‡∂ö‡∑í‡∂∫" (not considered)|
+-----------+----------------+----------------------------+



================================================================================
5. EMBEDDING GENERATION - DETAILED EXPLANATION
================================================================================

5.1 WHAT IS A TEXT EMBEDDING?
=============================

An embedding is a way to represent text as a list of numbers (a vector).
These numbers capture the "meaning" of the text.

SIMPLE ANALOGY:
Think of it like GPS coordinates for meaning.
- "Price increase" might be at coordinates [23.5, -45.2, ...]
- "Cost went up" has similar meaning, so similar coordinates [24.1, -44.8, ...]
- "Cat sat on mat" has different meaning, so different coordinates [-12.3, 67.8, ...]

5.2 HOW EMBEDDINGS WORK
=======================

The embedding model learns from millions of texts. It learns that:
- Words used in similar contexts have similar meanings
- "King - Man + Woman = Queen" (famous example)
- Synonyms have similar embeddings
- Antonyms have different embeddings

5.3 TECHNICAL DETAILS
=====================

EMBEDDING MODEL: Multilingual embedding model via OpenRouter API

VECTOR SIZE: 768 dimensions
- Each text is represented by 768 floating-point numbers
- More dimensions = more nuance captured

API CALL:
1. Send text to OpenRouter API
2. API processes with embedding model
3. Returns 768-dimensional vector

5.4 WHY USE EMBEDDINGS?
=======================

1. SEMANTIC SEARCH: Find texts with similar meaning
   - Search "fuel price rise" 
   - Find "petrol costs increased" (same meaning, different words)

2. LANGUAGE AGNOSTIC: Works across languages
   - Sinhala text and English text about same topic
   - Will have similar embeddings

3. EFFICIENT: Compare millions of texts quickly
   - Just compute distance between vectors
   - Much faster than string matching



================================================================================
6. PINECONE VECTOR DATABASE - DETAILED EXPLANATION
================================================================================

6.1 WHAT IS PINECONE?
=====================

Pinecone is a cloud-hosted vector database. It stores embeddings and
enables fast similarity search.

NORMAL DATABASE:
"Find all articles where country = 'Sri Lanka'"
Returns: Exact matches only

VECTOR DATABASE:
"Find articles similar to this claim about fuel prices"
Returns: Articles with similar meaning (not just same words)

6.2 HOW PINECONE WORKS
======================

STORING DATA:
1. Generate embedding for text
2. Send embedding + metadata to Pinecone
3. Pinecone indexes the vector for fast search

SEARCHING DATA:
1. Generate embedding for query
2. Send to Pinecone
3. Pinecone finds nearest vectors
4. Returns most similar documents

6.3 PINECONE STRUCTURE FOR THIS PROJECT
=======================================

INDEX NAME: sinhala-fake-news

NAMESPACES:
1. "dataset" - Labeled training data (~5,000 vectors)
2. "live_news" - Scraped news articles (~10,000 vectors)

STORED METADATA:
+------------------+------------------------------------------+
| Field            | Description                              |
+------------------+------------------------------------------+
| text             | Original text content                    |
| title            | News headline                            |
| source           | Lankadeepa, Twitter, etc.                |
| label            | true, false, or empty                    |
| entities         | NER results (persons, places)            |
| has_claim        | Boolean - contains claim indicator       |
| pub_date         | Publication date if available            |
+------------------+------------------------------------------+

6.4 QUERYING PINECONE
=====================

When a user submits a claim:

1. Generate embedding for the claim
2. Query Pinecone: "Find 10 most similar documents"
3. Pinecone returns documents with:
   - Similarity score (0 to 1)
   - Original text
   - Label (if from dataset namespace)
   - Source



================================================================================
7. DATA STATISTICS AND SUMMARY
================================================================================

7.1 FINAL DATASET COMPOSITION
=============================

+------------------+--------+--------+---------+---------------+
| Source           | TRUE   | FALSE  | TOTAL   | Percentage    |
+------------------+--------+--------+---------+---------------+
| Lankadeepa       | 2,500  | 0      | 2,500   | 50%           |
| Hiru News        | 1,000  | 0      | 1,000   | 20%           |
| NewsPosts        | 500    | 0      | 500     | 10%           |
| FakeNews_Annot.  | 250    | 250    | 500     | 10%           |
| Twitter          | 250    | 250    | 500     | 10%           |
+------------------+--------+--------+---------+---------------+
| TOTAL            | 4,500  | 500    | 5,000   | 100%          |
+------------------+--------+--------+---------+---------------+


7.2 LABEL DISTRIBUTION
======================

TRUE:  4,500 records (90%)
FALSE: 500 records (10%)

VISUAL REPRESENTATION:
TRUE:  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 90%
FALSE: ‚ñà‚ñà‚ñà‚ñà‚ñà 10%


7.3 TEXT LENGTH STATISTICS
==========================

+------------------+--------+--------+--------+
| Metric           | Min    | Avg    | Max    |
+------------------+--------+--------+--------+
| Characters       | 31     | 450    | 3,500  |
| Words            | 5      | 75     | 600    |
| Sentences        | 1      | 8      | 45     |
+------------------+--------+--------+--------+


7.4 CLASS IMBALANCE DISCUSSION
==============================

The dataset is imbalanced (90% TRUE, 10% FALSE). This is actually
realistic because in the real world, most news is true.

HOW THE SYSTEM HANDLES THIS:
1. Does not use pure classification approach
2. Uses evidence-based verification
3. Research Agent searches for corroborating sources
4. Judge Agent weighs evidence, not just majority class



================================================================================
8. ETHICAL CONSIDERATIONS AND DATA QUALITY
================================================================================

8.1 DATA PRIVACY
================

- All data is from public sources
- No private messages or personal data
- Twitter usernames are not stored (only text)
- News articles are public content

8.2 BIAS CONSIDERATIONS
=======================

ACKNOWLEDGED BIASES:
1. Mainstream media bias: TRUE labels mostly from major news sources
2. Urban bias: More content from Colombo-based sources
3. Political bias: Each news source has editorial stance
4. Time bias: Data from specific time periods (2019, 2023)

MITIGATION:
- Multiple sources used
- Both government and independent sources included
- Acknowledged limitations in thesis

8.3 DATA QUALITY MEASURES
=========================

MEASURE 1: MINIMUM LENGTH
- Filter texts < 30 characters
- Ensures meaningful content

MEASURE 2: DUPLICATE REMOVAL
- Check for exact duplicates
- Remove to prevent bias

MEASURE 3: ENCODING CHECK
- Verify proper Sinhala Unicode
- Fix encoding issues

MEASURE 4: MANUAL VERIFICATION
- Fake labels verified by human annotators
- Multiple annotators where possible



================================================================================
                        END OF DATA SECTION
================================================================================
