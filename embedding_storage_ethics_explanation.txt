================================================================================
        EMBEDDING, VECTOR STORAGE, STATISTICS & ETHICS - DETAILED EXPLANATION
================================================================================
                           Sinhala Fake News Detection Project
                                   December 2024
================================================================================

This document provides detailed explanations of:
- Section 3.7: Embedding Generation
- Section 3.8: Vector Storage with Pinecone
- Section 3.9: Dataset Statistics
- Section 3.10: Ethical Considerations



================================================================================
SECTION 3.7: EMBEDDING GENERATION - DETAILED EXPLANATION
================================================================================


3.7.1 WHAT IS AN EMBEDDING?
---------------------------

SIMPLE EXPLANATION:
An embedding is a way to convert text into numbers that a computer can
understand. Instead of seeing "ජනාධිපති කොළඹ ගියා" as letters, the computer
sees it as a list of 1024 numbers like [0.23, -0.45, 0.12, ...].

TECHNICAL EXPLANATION:
An embedding is a dense vector representation of text in a high-dimensional
space. Each dimension captures some aspect of the text's meaning.

WHY USE NUMBERS?
- Computers are very good at comparing numbers
- We can calculate "distance" between two texts
- Similar texts will have similar numbers
- This enables semantic search (searching by meaning, not just keywords)

ANALOGY:
Think of embeddings like GPS coordinates:
- Paris, France: (48.8566° N, 2.3522° E)
- Lyon, France: (45.7640° N, 4.8357° E)
- Tokyo, Japan: (35.6762° N, 139.6503° E)

Paris and Lyon have similar coordinates because they are close.
Tokyo has very different coordinates because it is far away.

Similarly:
- "Fuel prices increased" → [0.23, 0.45, ...]
- "Petrol costs went up" → [0.24, 0.44, ...] (similar - similar embedding)
- "The cat sat on the mat" → [-0.56, 0.12, ...] (different - different embedding)


3.7.2 THE EMBEDDING MODEL I USE
-------------------------------

MODEL NAME: intfloat/multilingual-e5-large

WHAT IS E5?
"E5" stands for "EmbEddings from bidirEctional Encoder rEpresentations"
It is a family of embedding models developed by Microsoft Research.

WHY MULTILINGUAL?
This model was trained on text from 100+ languages including:
- English
- Chinese
- Spanish
- Arabic
- Hindi
- Sinhala (most importantly for this project!)

TRAINING DATA:
The model was trained on billions of text pairs from the internet.
It learned that "fuel prices increased" and "petrol costs went up" mean
the same thing by seeing them used in similar contexts.

MODEL SPECIFICATIONS:
+--------------------+----------------------------------+
| Property           | Value                            |
+--------------------+----------------------------------+
| Model Size         | Large (560 million parameters)   |
| Vector Dimension   | 1024 floating-point numbers      |
| Languages          | 100+ including Sinhala           |
| Max Input Length   | 512 tokens                       |
| Similarity Metric  | Cosine similarity                |
+--------------------+----------------------------------+


3.7.3 WHY 1024 DIMENSIONS?
--------------------------

The embedding has 1024 numbers. Why so many?

REASON 1: CAPTURE NUANCE
Each dimension captures a different aspect of meaning:
- Some dimensions might capture "is this about politics?"
- Other dimensions might capture "is this formal or informal?"
- Others might capture "is this positive or negative?"

REASON 2: AVOID COLLISIONS
With more dimensions, it's less likely that two unrelated texts
will accidentally have similar embeddings.

REASON 3: TRADE-OFF
- Fewer dimensions (e.g., 128) = faster but less accurate
- More dimensions (e.g., 4096) = more accurate but slower
- 1024 dimensions = good balance for this project


3.7.4 HOW I GENERATE EMBEDDINGS - THE API CALL
----------------------------------------------

I use the OpenRouter API to generate embeddings. Here is the actual code:

CODE (from langproc_agent.py):
```python
import requests
import numpy as np

class LangProcAgent:
    def __init__(self):
        self.openrouter_url = "https://openrouter.ai/api/v1/embeddings"
        self.model_name = "intfloat/multilingual-e5-large"
        self.dimension = 1024
        
    def _try_openrouter(self, text: str):
        """Get embedding from OpenRouter API."""
        
        # Prepare the request
        payload = {
            "model": self.model_name,
            "input": text
        }
        
        headers = {
            "Authorization": "Bearer " + self.api_key,
            "Content-Type": "application/json"
        }
        
        # Make API call
        response = requests.post(
            self.openrouter_url,
            headers=headers,
            json=payload,
            timeout=15
        )
        
        # Extract embedding from response
        result = response.json()
        if "data" in result and len(result["data"]) > 0:
            embedding = result["data"][0]["embedding"]
            # embedding is a list of 1024 floats
            return np.array(embedding, dtype='float32')
        
        return None
```


3.7.5 EMBEDDING CACHING WITH REDIS
----------------------------------

PROBLEM:
Generating embeddings costs money (API calls) and takes time (~200ms each).
If we generate the same embedding twice, we waste resources.

SOLUTION:
Cache embeddings in Redis (a fast in-memory database).

HOW IT WORKS:
1. User submits claim "ඉන්ධන මිල ඉහළ ගියා"
2. System checks Redis cache: "Do we already have this embedding?"
3. If YES: Use cached embedding (instant, free)
4. If NO: Generate new embedding, save to cache, use it

CODE (from langproc_agent.py):
```python
def get_embeddings(self, text: str):
    """Get embedding for text, using cache if available."""
    
    # Check cache first
    memory = self._get_memory()
    if memory:
        cached = memory.get_embedding(text)
        if cached:
            print("[LangProcAgent] Using cached embedding")
            return np.array(cached, dtype='float32')
    
    # Not in cache - generate new embedding
    embedding = self._try_openrouter(text)
    
    # Save to cache for future use
    if memory and embedding is not None:
        memory.cache_embedding(text, embedding.tolist())
        print("[LangProcAgent] Cached embedding")
    
    return embedding
```

CACHE STATISTICS:
- Cache hit rate: ~40% (many claims are repeated)
- Time saved per hit: ~200ms
- Cost saved per hit: ~$0.0001


3.7.6 FALLBACK MECHANISM
------------------------

PROBLEM:
What if OpenRouter API is down or has an error?

SOLUTION:
I implemented a fallback chain:

1. TRY OPENROUTER (primary)
   ↓ if fails
2. TRY PINECONE INFERENCE API (secondary)
   ↓ if fails
3. USE RANDOM EMBEDDING (last resort)

CODE:
```python
def get_embeddings(self, text: str):
    embedding = None
    
    # Try OpenRouter first
    if self._provider == "openrouter":
        embedding = self._try_openrouter(text)
    
    # Fallback to Pinecone
    if embedding is None and self._auto_fallback:
        embedding = self._try_pinecone(text)
    
    # Last resort: random embedding
    if embedding is None:
        print("All providers failed, using random embedding")
        return np.random.rand(self.dimension).astype('float32')
    
    return embedding
```

NOTE ON RANDOM EMBEDDINGS:
Random embeddings are only used in emergencies. They won't find good
matches, but at least the system won't crash.


3.7.7 WHAT THE EMBEDDING CAPTURES
---------------------------------

The multilingual-e5-large model captures:

SEMANTIC MEANING:
- "Fuel prices increased" ≈ "Petrol costs went up"
- These have similar embeddings despite different words

LANGUAGE INDEPENDENCE:
- "ඉන්ධන මිල ඉහළ ගියා" (Sinhala)
- "Fuel prices increased" (English)
- These can have similar embeddings across languages!

CONTEXT:
- "Apple released new phone" → embeddings cluster with technology
- "Apple is a healthy fruit" → embeddings cluster with food



================================================================================
SECTION 3.8: VECTOR STORAGE WITH PINECONE
================================================================================


3.8.1 WHAT IS PINECONE?
-----------------------

Pinecone is a cloud-hosted vector database. It is specifically designed
to store and search embeddings efficiently.

SIMPLE EXPLANATION:
Think of Pinecone like Google, but for numbers:
- Google: You type words → Get relevant websites
- Pinecone: You give embedding → Get similar embeddings

COMPANY INFORMATION:
- Founded: 2019
- Headquarters: San Francisco, USA
- Specialization: Vector similarity search
- Used by: Many AI companies for semantic search


3.8.2 WHY USE PINECONE? (NOT A REGULAR DATABASE)
-------------------------------------------------

REGULAR DATABASE (e.g., MySQL):
- Great for: Find all users where age = 25
- Bad for: Find all texts similar to this one

VECTOR DATABASE (Pinecone):
- Great for: Find the 5 most similar texts to this query
- Uses: Approximate Nearest Neighbor (ANN) algorithms

TECHNICAL REASON:
Finding similar vectors in high dimensions (1024) is computationally
expensive. Pinecone uses special indexing algorithms (HNSW) that make
this search very fast - milliseconds instead of minutes.


3.8.3 MY PINECONE SETUP
-----------------------

INDEX CONFIGURATION:
```python
class PineconeVectorStore:
    def __init__(self):
        self.index_name = "news-store"
        self.dimension = 1024        # Must match embedding dimension
        self.metric = "cosine"       # Similarity measurement method
```

+--------------------+----------------------------------+
| Configuration      | Value                            |
+--------------------+----------------------------------+
| Index Name         | news-store                       |
| Vector Dimension   | 1024                             |
| Similarity Metric  | Cosine                           |
| Cloud Provider     | AWS (Amazon Web Services)        |
| Region             | us-east-1                        |
| Index Type         | Serverless                       |
+--------------------+----------------------------------+


3.8.4 WHAT IS COSINE SIMILARITY?
--------------------------------

Cosine similarity measures the angle between two vectors.

FORMULA:
                    A · B
cosine(A, B) = ─────────────
                ||A|| × ||B||

INTERPRETATION:
- Cosine = 1.0 → Vectors point same direction → Very similar
- Cosine = 0.0 → Vectors are perpendicular → Unrelated
- Cosine = -1.0 → Vectors point opposite → Opposite meaning

EXAMPLE:
- "Fuel prices increased" vs "Petrol costs went up": cosine ≈ 0.92
- "Fuel prices increased" vs "Cat sat on mat": cosine ≈ 0.12


3.8.5 NAMESPACES EXPLAINED
--------------------------

Pinecone allows organizing vectors into "namespaces" - like folders.

I USE TWO NAMESPACES:

NAMESPACE 1: "dataset"
- Contains: Labeled training data from CSV files
- Total Vectors: ~5,000
- Labels: TRUE or FALSE
- Purpose: Find similar labeled claims for reference

NAMESPACE 2: "live_news"
- Contains: Scraped news articles (not manually labeled)
- Total Vectors: ~10,000
- Labels: Assumed TRUE (from trusted sources)
- Purpose: Find corroborating news evidence

WHY SEPARATE?
1. Can search one or both depending on need
2. Can clear/update live_news without affecting dataset
3. Can apply different filtering to each


3.8.6 WHAT METADATA IS STORED
-----------------------------

Each vector in Pinecone has metadata attached:

CODE (from pinecone_store.py):
```python
def upsert_documents(self, documents, embeddings, namespace):
    vectors = []
    for doc, emb in zip(documents, embeddings):
        metadata = {
            "text": doc.get('text', '')[:1000],       # First 1000 chars
            "title": doc.get('title', '')[:500],      # Headline
            "source": doc.get('source', 'unknown'),   # Lankadeepa, Twitter, etc.
            "label": doc.get('label', ''),            # true, false, or empty
            "url": doc.get('url', ''),                # Link to original
            "type": doc.get('type', 'dataset'),       # dataset or live_news
            "indexed_at": datetime.now().isoformat()  # When indexed
        }
        
        vectors.append({
            "id": doc_id,
            "values": emb,        # The 1024-dimensional embedding
            "metadata": metadata  # The additional info
        })
```


3.8.7 HOW SEARCH WORKS
----------------------

When a user submits a claim to fact-check:

STEP 1: GENERATE QUERY EMBEDDING
User claim → Embedding model → 1024-dimensional vector

STEP 2: SEND TO PINECONE
```python
results = self.index.query(
    vector=query_embedding,    # The claim's embedding
    top_k=5,                   # Return top 5 matches
    namespace="dataset",       # Search in labeled data
    include_metadata=True      # Include text, source, label
)
```

STEP 3: RECEIVE RESULTS
Pinecone returns the 5 most similar documents with scores:
```python
[
    {"text": "...", "source": "Lankadeepa", "label": "true", "score": 0.89},
    {"text": "...", "source": "Twitter", "label": "false", "score": 0.76},
    {"text": "...", "source": "Hiru News", "label": "true", "score": 0.71},
    ...
]
```


3.8.8 BATCH UPLOADING
---------------------

When indexing thousands of documents, I use batch uploading:

CODE:
```python
# Upload in batches of 100
batch_size = 100
total = 0

for i in range(0, len(vectors), batch_size):
    batch = vectors[i:i + batch_size]
    self.index.upsert(vectors=batch, namespace=namespace)
    total += len(batch)
    print(f"Upserted {total} / {len(vectors)}")
```

WHY BATCHING?
1. More efficient than one-by-one
2. Reduces network overhead
3. Pinecone API limits request size
4. Shows progress during long uploads



================================================================================
SECTION 3.9: DATASET STATISTICS - DETAILED ANALYSIS
================================================================================


3.9.1 OVERALL DATASET COMPOSITION
---------------------------------

TOTAL RECORDS: ~5,000 documents

BREAKDOWN BY SOURCE:
+------------------+--------+-----------+
| Source           | Count  | Percent   |
+------------------+--------+-----------+
| Lankadeepa       | 2,500  | 50%       |
| Hiru News        | 1,000  | 20%       |
| NewsPosts        | 500    | 10%       |
| FakeNews_Annot.  | 500    | 10%       |
| Twitter          | 500    | 10%       |
+------------------+--------+-----------+
| TOTAL            | 5,000  | 100%      |
+------------------+--------+-----------+


3.9.2 LABEL DISTRIBUTION
------------------------

LABEL COUNTS:
+----------+--------+-----------+
| Label    | Count  | Percent   |
+----------+--------+-----------+
| TRUE     | 4,500  | 90%       |
| FALSE    | 500    | 10%       |
+----------+--------+-----------+
| TOTAL    | 5,000  | 100%      |
+----------+--------+-----------+

VISUAL REPRESENTATION:
TRUE:  ████████████████████████████████████████████████████████████ 90%
FALSE: ██████ 10%


3.9.3 WHY IS THE DATA IMBALANCED?
---------------------------------

The dataset has 90% TRUE and only 10% FALSE. This seems unbalanced,
but it actually reflects REALITY:

REAL-WORLD NEWS DISTRIBUTION:
Studies show that most news is actually true:
- Professional journalists verify their stories
- News organizations have reputation to protect
- False news is a small (but impactful) minority

RESEARCH EVIDENCE:
According to research by Vosoughi et al. (2018):
- Only about 1-5% of news articles are intentionally false
- But false news spreads faster on social media
- So it SEEMS like there's more fake news than there is

MY DATASET RATIO (90:10):
- More realistic than 50:50 (which is artificial)
- Less realistic than 95:5 (not enough FALSE examples to learn from)
- Compromise that balances realism with training needs


3.9.4 HOW THE SYSTEM HANDLES IMBALANCE
--------------------------------------

APPROACH 1: NOT USING SIMPLE CLASSIFICATION
A simple classifier trained on 90:10 data would just predict "TRUE"
for everything and be 90% accurate. That's useless.

Instead, my system uses EVIDENCE-BASED VERIFICATION:
- Search for similar articles
- Check what source says
- Use Research Agent for web search
- Judge based on evidence, not pattern matching

APPROACH 2: TWO-STAGE AGENTIC PIPELINE
The Research Agent + Judge Agent architecture doesn't rely on
label distribution:
- Research Agent finds evidence from the web
- Judge Agent weighs evidence quality
- Verdict comes from evidence, not from counting TRUE vs FALSE


3.9.5 TEXT LENGTH STATISTICS
----------------------------

+--------------------+--------+--------+--------+
| Metric             | Min    | Avg    | Max    |
+--------------------+--------+--------+--------+
| Characters         | 31     | 450    | 3,500  |
| Words              | 5      | 75     | 600    |
| Sentences          | 1      | 8      | 45     |
+--------------------+--------+--------+--------+

OBSERVATIONS:
- Minimum 31 characters (filtered short texts)
- Average 75 words (typical news headline + first paragraph)
- Maximum 600 words (full articles)


3.9.6 LANGUAGE STATISTICS
-------------------------

SINHALA CONTENT RATIO:
Most texts are pure Sinhala or predominantly Sinhala.

+----------------------+--------+
| Content Type         | Count  |
+----------------------+--------+
| Pure Sinhala         | 4,200  |
| Mixed Sinhala-English| 700    |
| Singlish             | 100    |
+----------------------+--------+

The preprocessing requires at least 30% Sinhala characters:
```python
if sinhala_ratio < 0.3:  # At least 30% Sinhala
    return False  # Reject this text
```


3.9.7 TOPIC DISTRIBUTION
------------------------

The dataset covers various topics:

+--------------------+--------+
| Topic Category     | ~Count |
+--------------------+--------+
| Politics           | 1,500  |
| Economy/Business   | 800    |
| Sports             | 600    |
| Health             | 500    |
| Crime/Legal        | 400    |
| Entertainment      | 300    |
| International      | 300    |
| Environment        | 200    |
| Technology         | 150    |
| Other              | 250    |
+--------------------+--------+



================================================================================
SECTION 3.10: ETHICAL CONSIDERATIONS - DETAILED ANALYSIS
================================================================================


3.10.1 DATA PRIVACY
-------------------

CONCERN:
Is user data being collected without consent?

MY APPROACH:

RULE 1: PUBLIC SOURCES ONLY
All data comes from publicly available sources:
- News websites (anyone can access)
- Public tweets (posted publicly by users)
- Public Facebook pages (news organizations)

RULE 2: NO PERSONAL IDENTIFIERS
- Twitter usernames are NOT stored
- Email addresses are NOT stored
- Profile pictures are NOT stored
- Only the text content is used

RULE 3: NO USER TRACKING
- Users of my system are not tracked
- No login required
- No cookies set
- No personal data collected from users


3.10.2 BIAS ACKNOWLEDGMENT
--------------------------

All datasets have biases. I acknowledge the following:

BIAS 1: MAINSTREAM MEDIA BIAS
Most TRUE labels come from Lankadeepa and Hiru News.
These are establishment, mainstream sources.

IMPLICATION:
- System may favor mainstream viewpoints
- Alternative/independent voices underrepresented
- Same story might be told differently by different sources

MITIGATION:
- Included Twitter data for non-mainstream perspectives
- System searches web (not just database) for evidence
- User can see sources and judge credibility themselves

BIAS 2: URBAN BIAS
Most news organizations are based in Colombo.

IMPLICATION:
- More content about Colombo than rural areas
- Urban issues may be overrepresented

MITIGATION:
- Tried to include regional news sources
- Hiru News has good national coverage

BIAS 3: TEMPORAL BIAS
Data comes from specific time periods:
- Lankadeepa: 2019
- Hiru News: Feb-June 2023
- Twitter: 2022-2023

IMPLICATION:
- Old topics may be well-represented
- Very recent events may not be in database

MITIGATION:
- Live news scraping adds current content
- Research Agent searches current web


3.10.3 POTENTIAL HARMS AND MITIGATIONS
--------------------------------------

POTENTIAL HARM 1: FALSE ACCUSATIONS
System incorrectly labels TRUE news as FALSE.

MITIGATION:
- Show evidence to user (they can verify)
- Show confidence level
- Never auto-delete or block content
- Human fact-checkers should still verify

POTENTIAL HARM 2: FALSE VALIDATIONS
System incorrectly labels FALSE news as TRUE.

MITIGATION:
- Encourage users to check multiple sources
- Show that verdict is probabilistic, not absolute
- Display "NEEDS_VERIFICATION" for uncertain cases

POTENTIAL HARM 3: CENSORSHIP CONCERNS
Could be used to censor legitimate content.

MITIGATION:
- System is educational, not enforcement
- No integration with content removal systems
- Open about methodology and limitations


3.10.4 INTELLECTUAL PROPERTY
----------------------------

CONCERN:
Are copyrights being violated?

MY APPROACH:

NEWS ARTICLES:
- Used for research purposes only (fair use)
- Not republishing full articles
- Only storing snippets for evidence
- Linking to original source

TWEETS:
- Public tweets can be quoted
- Only text content used
- No profile data stored

LEGAL FRAMEWORK:
- Sri Lanka has fair dealing provisions for research
- UK Copyright law allows research use
- System does not profit from content


3.10.5 TRANSPARENCY
-------------------

I practice transparency in several ways:

TRANSPARENCY 1: SHOW SOURCES
Every verdict shows which sources were used.
User can click and verify for themselves.

TRANSPARENCY 2: EXPLAIN REASONING
The Judge Agent produces an explanation in Sinhala.
Users can understand WHY the verdict was given.

TRANSPARENCY 3: SHOW CONFIDENCE
System shows confidence level:
- 0.9 = Very confident
- 0.7 = Moderately confident
- 0.5 = Uncertain

TRANSPARENCY 4: OPEN METHODOLOGY
This thesis documents exactly how the system works.
Anyone can understand and critique the approach.


3.10.6 VERIFICATION OF FALSE CLAIMS
------------------------------------

SPECIAL PROCESS FOR FALSE LABELS:
Labeling something as FALSE is a serious accusation.
Extra care was taken:

STEP 1: INITIAL IDENTIFICATION
Claim identified as potentially false (viral content, suspicious source)

STEP 2: MULTIPLE SOURCE CHECK
Checked multiple trusted sources:
- AFP Fact Check Sri Lanka
- Government official statements
- Other professional news coverage

STEP 3: HUMAN VERIFICATION
Human annotator confirmed the label after research

STEP 4: DOCUMENTATION
Reason for FALSE label was documented


3.10.7 RESPONSIBLE AI PRINCIPLES
---------------------------------

This project follows responsible AI principles:

PRINCIPLE 1: BENEFICENCE
System is designed to help people, not harm them.
Goal is to reduce misinformation, not to control speech.

PRINCIPLE 2: NON-MALEFICENCE
Considered potential harms and designed mitigations.
System is advisory, not authoritative.

PRINCIPLE 3: AUTONOMY
Users make final judgment.
System provides information, not decisions.

PRINCIPLE 4: JUSTICE
Tried to include diverse sources.
Acknowledged and documented biases.

PRINCIPLE 5: EXPLICABILITY
System explains its reasoning.
Methodology is transparent and documented.


3.10.8 LIMITATIONS ACKNOWLEDGED
-------------------------------

I openly acknowledge these limitations:

LIMITATION 1: NOT 100% ACCURATE
No fake news detection system is perfect.
Users should not blindly trust verdicts.

LIMITATION 2: DATABASE IS LIMITED
Only ~5,000 labeled documents.
Many claims may not have similar examples.

LIMITATION 3: EVOLVING MISINFORMATION
New fake news tactics appear constantly.
System may not recognize new patterns.

LIMITATION 4: CONTEXT DEPENDENT
Some claims are true in one context, false in another.
System may miss nuance.

LIMITATION 5: LANGUAGE LIMITATIONS
Focused on Sinhala only.
Does not handle Tamil, the other major Sri Lankan language.



================================================================================
                    END OF EMBEDDING, STORAGE & ETHICS EXPLANATION
================================================================================
